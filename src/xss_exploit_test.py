import requests
import os
from dotenv import load_dotenv
from urllib.parse import urlparse
import socket

from scapy.all import *
from scapy.layers.http import HTTPRequest
import socket
from selenium import webdriver

#не свечу свои данные в коде
load_dotenv()
my_id = os.getenv("MY_ID")
username = os.getenv("USERNAME")
password = os.getenv("PSWRD")

#трафик перхвачу через scapy, поэтому http
base_url = f"http://google-gruyere.appspot.com/" 

#Два вида xss атак из условий ДЗ
payload_1 = "<script>alert('test')</script>"
payload_2 = "<img src=\"nonexistent.jpg\" onerror=\"alert('XSS')\">"
#ещё один из сети
payload_3= "<script>document.body.innerHTML = '<h1>Hacked!</h1><p>This site has been compromised.</p>';</script>"

#Для сниффера
#определяю IP для сниффера
site_ip = socket.gethostbyname(urlparse(base_url).netloc)
#количество перехватываемых пакетов
pkg_count = 100
#Интерфейс для перхват, лучше конечно спрашивать, но для теста и ДЗ будет жестко выбран
interface = "eth0"

#код для вывода статусов ответа
def handle_status(code):
    if code == 200:
        print("Успешно!")
        return
    #ошибки
    errors = {
        404: "Ресурс не найден. Проверьте URL.",
        429: "Слишком много запросов. Подождите.",
        403: "Доступ запрещен.",
        401: "Требуется авторизация.",
    }
    
    if code in errors:
        raise Exception(f"{code}: {errors[code]}")
    elif code >= 500:
        raise Exception(f"{code}: Ошибка сервера.")
    else:
        raise Exception(f"{code}: Ошибка запроса.")

#функция отправки post
def post_req(url, data = None, auth = None, timeout = None):
    response = requests.post(
        url,
        data=data,
        auth=auth,
        timeout=timeout
    )
    handle_status(response.status_code)
    return response 


def simple_capture(site_url, count=20):
    """Самая простая функция захвата трафика на сайт"""
    
    # Получаем IP сайта
    site_ip = socket.gethostbyname(urlparse(base_url).netloc)
    
    print(f"Захватываю трафик на {site_url} ({site_ip})...")
    
    # Захватываем пакеты
    packets = sniff(
        filter=f"host {site_ip} and tcp port 80",
        count=count,
        timeout=30  # Таймаут 30 секунд
    )
    
    # Анализируем захваченные пакеты
    for packet in packets:
        if packet.haslayer(HTTPRequest):
            http = packet[HTTPRequest]
            print(f"\nGET запрос к {site_url}")
            print(f"  От: {packet[IP].src}")
            if hasattr(http, 'Path'):
                print(f"  Путь: {http.Path.decode()}")
    
    print(f"\nЗахвачено {len(packets)} пакетов")



#ЭТАП 1 Логигнимся
#print(f"Логинимся на сайте {base_url}")
#post_req(
#    base_url + my_id + "/login",
#    auth=(username, password)
#)

def xss_explot_result_print(condition, text):
    if condition in text:
        print("XSS сработал")
    else:
        print("XSS не сработал")
    return None

capture_thread = threading.Thread(
    target=simple_capture,
    args=(base_url, 5),  # 20 пакетов максимум
    daemon=True  # Поток завершится с основной программой
)
    
   # Запускаем поток
#capture_thread.start()
#print("⏳ Запускаю сниффер, жду 2 секунды...")
#time.sleep(2)

#ЭТАП 1 Логигнимся
#print(f"Логинимся на сайте {base_url}")
#post_req(
#    base_url + my_id + "/login",
#    auth=(username, password)
#)


#Тесты из задания
#response = requests.get(f"{base_url}{my_id}/{payload_1}")
#xss_explot_result_print("test", response.text)

#response = requests.get(f"{base_url}{my_id}/{payload_2}")
#xss_explot_result_print("onerror=", response.text)

#response = requests.get(f"{base_url}{my_id}/{payload_3}")
#xss_explot_result_print("Hacked!", response.text)

def hybrid_gruyere_workflow():
     # ===== 2. ДЕЙСТВИЯ через Selenium =====
    print("\n2. Создаю сниппет через Selenium...")
    
    driver = webdriver.Chrome()
    
    try:
        # Авторизация
        driver.get(f"{base_url}{my_id}/login")
        driver.find_element(By.NAME, "uid").send_keys("username")
        driver.find_element(By.NAME, "pw").send_keys("password")
        driver.find_element(By.XPATH, "//input[@type='submit']").click()
        
        # ===== 3. АНАЛИЗ динамической страницы =====
        # Получаем HTML после авторизации
        dynamic_html = driver.page_source
        soup2 = BeautifulSoup(dynamic_html, 'html.parser')
        
        # Проверяем авторизацию
        user_elem = soup2.find('span', class_='menu-user')
        if user_elem:
            print(f"Авторизован как: {user_elem.text}")
        
        # ===== 4. СОЗДАНИЕ сниппета =====
        driver.get(f"{base_url}/newsnippet.gtl")
        driver.find_element(By.NAME, "snippet").send_keys("Мой сниппет")
        driver.find_element(By.XPATH, "//input[@value='Submit']").click()
        
        # ===== 5. ПРОВЕРКА через BeautifulSoup =====
        driver.get(f"{base_url}/snippets.gtl")
        final_html = driver.page_source
        soup3 = BeautifulSoup(final_html, 'html.parser')
        
        if "Мой сниппет" in soup3.text:
            print("✅ Сниппет успешно создан!")
        else:
            print("❌ Сниппет не найден")
            
    finally:
        driver.quit()