import requests
from bs4 import BeautifulSoup
import json
import re  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç re
import os
from dotenv import load_dotenv
from urllib.parse import urljoin  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è URL

load_dotenv()
my_id = os.getenv("MY_ID")

def filter_id(text):
    """–ó–∞–º–µ–Ω—è–µ—Ç my_id –Ω–∞ [FILTERED] –≤ —Ç–µ–∫—Å—Ç–µ"""
    if not text or not my_id:
        return text
    return text.replace(my_id, '[FILTERED]')

def filter_url(url):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç ID –≤ URL"""
    if not url or not my_id:
        return url
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ my_id, –∞ –Ω–µ –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ª—é–±—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö —á–∏—Å–µ–ª
    return url.replace(my_id, '[FILTERED]')

def filter_id_in_sitemap(data, my_id):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∑–∞–º–µ–Ω—è–µ—Ç my_id –Ω–∞ [FILTERED] –≤–æ –≤—Å–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ site_map
    """
    if not my_id:  # –µ—Å–ª–∏ my_id –ø—É—Å—Ç–æ–π, –Ω–µ—á–µ–≥–æ –∑–∞–º–µ–Ω—è—Ç—å
        return data
    
    if isinstance(data, dict):
        filtered_dict = {}
        for key, value in data.items():
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª—é—á–∏ (–µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç—Ä–æ–∫–∏)
            filtered_key = filter_id(key) if isinstance(key, str) else key
            filtered_value = filter_id_in_sitemap(value, my_id)
            filtered_dict[filtered_key] = filtered_value
        return filtered_dict
    
    elif isinstance(data, list):
        return [filter_id_in_sitemap(item, my_id) for item in data]
    
    elif isinstance(data, str):
        return filter_id(data)
    
    else:  # —á–∏—Å–ª–∞, –±—É–ª–µ–≤—ã –∑–Ω–∞—á–µ–Ω–∏—è, None
        return data

def create_site_map(base_url):
    """–°–æ–∑–¥–∞–µ—Ç –∫–∞—Ä—Ç—É —Å–∞–π—Ç–∞ —Å–æ –≤—Å–µ–º–∏ —Ñ–æ—Ä–º–∞–º–∏ –∏ –ø–æ–ª—è–º–∏"""

    site_map = {
        "base_url": filter_url(base_url),  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É
        "pages": {}
    }
    
    visited = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    
    def crawl(url, depth=0, max_depth=3):
        if depth > max_depth or url in visited:
            return
        
        # –í—ã–≤–æ–¥–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL –≤ –ª–æ–≥–∞—Ö
        print(f"{'  ' * depth}üìÑ {url}")
        visited.add(url)
        
        try:
            resp = requests.get(url)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            page_info = {
                "forms": [],
                "links": [],
                "url_params": []
            }
            
            # –§–æ—Ä–º—ã - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ —Å–±–æ—Ä–µ
            for form in soup.find_all('form'):
                form_info = {
                    "action": filter_id(form.get('action')),
                    "method": filter_id(form.get('method', 'GET')),
                    "inputs": []
                }
                
                for inp in form.find_all(['input', 'textarea', 'select']):
                    form_info["inputs"].append({
                        "name": filter_id(inp.get('name')),
                        "type": filter_id(inp.get('type', inp.name)),
                        "id": filter_id(inp.get('id'))
                    })
                
                page_info["forms"].append(form_info)
            
            # –°—Å—ã–ª–∫–∏ - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ —Å–±–æ—Ä–µ
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.startswith('/') or base_url in href:
                    page_info["links"].append({
                        "text": filter_id(link.text.strip()[:50]),
                        "href": filter_id(href)
                    })
            
            # URL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if '?' in url:
                params = url.split('?')[1].split('&')
                filtered_params = []
                for param in params:
                    if '=' in param:
                        key, value = param.split('=', 1)
                        filtered_params.append(f"{key}={filter_id(value)}")
                    else:
                        filtered_params.append(filter_id(param))
                page_info["url_params"] = filtered_params
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º URL –∫–ª—é—á–æ–º
            filtered_url_key = filter_url(url)
            site_map["pages"][filtered_url_key] = page_info
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏
            for link in page_info["links"][:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É
                next_url = urljoin(url, link["href"])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º urljoin –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                if base_url in next_url:
                    crawl(next_url, depth + 1, max_depth)
                    
        except Exception as e:
            print(f"{'  ' * depth}‚ùå –û—à–∏–±–∫–∞: {e}")
    
    crawl(base_url)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ–≥–æ site_map (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    filtered_site_map = filter_id_in_sitemap(site_map, my_id)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    with open('gruyere_sitemap.json', 'w', encoding='utf-8') as f:
        json.dump(filtered_site_map, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ –ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ gruyere_sitemap.json")
    print(f"üîí MY_ID '{my_id}' –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ '[FILTERED]' –≤–æ –≤—Å–µ—Ö –º–µ—Å—Ç–∞—Ö")
    
    return filtered_site_map 

# –ó–∞–ø—É—Å–∫
load_dotenv()
my_id = os.getenv("MY_ID")
url = f"https://google-gruyere.appspot.com/{my_id}/"
create_site_map(url)